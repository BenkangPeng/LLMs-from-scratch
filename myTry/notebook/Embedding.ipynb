{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt: str, tokenizer: tiktoken.Encoding, max_length, stride):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            txt: 输入的文本\n",
    "            tokenizer: 分词器\n",
    "            max_length: 单个input_ids最大长度\n",
    "            stride: 窗口步长\n",
    "        \"\"\"\n",
    "        #用input_ids去预测output_ids(自监督学习)\n",
    "        self.input_ids = [] #训练集\n",
    "        self.output_ids = [] #labels\n",
    "        \n",
    "        # encode the token using tiktoken\n",
    "        # every token gets its unique id\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "\n",
    "        for i in range(0, len(token_ids)-max_length-1, stride):\n",
    "            # use input_chunk to predict the next token\n",
    "            # use output_chunk as lables\n",
    "            # ipput_chunk与output_chunk是一个平移关系\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            output_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.output_ids.append(torch.tensor(output_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.output_ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1807, 3619,  402,  271]), tensor([ 3619,   402,   271, 10899]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"../ch02/01_main-chapter-code/the-verdict.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "dataset = GPTDatasetV1(text, tokenizer, 4, 4)\n",
    "\n",
    "print(dataset[1])\n",
    "\n",
    "# 1807 --->(predict)  3619\n",
    "# 1807,3619 --->(predict)  402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(txt, batch_size=4, max_length=256, stride=128, \n",
    "                      shuffle=False, drop_last=True, num_workers=0):\n",
    "    \n",
    "    #instantiate the tokenizer of gpt2\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size, #load 4 input_chunk and output_chunk once time\n",
    "        shuffle,\n",
    "        drop_last = drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "max_length = 4\n",
    "stride = 4\n",
    "\n",
    "dataloader = create_dataloader(text, 4, 4, 4)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.5675e-02,  3.2620e-01,  7.7150e-01,  1.4889e+00, -6.3363e-01,\n",
      "         -1.4542e-01, -1.7267e+00,  9.8608e-01, -2.4061e+00,  1.3661e-01,\n",
      "         -2.3796e-01, -9.6542e-01,  3.0222e-01, -2.8087e-01,  1.3109e+00,\n",
      "         -9.9792e-03,  1.2938e+00, -2.2220e-01,  1.3804e+00,  4.0176e-01,\n",
      "         -9.0445e-02, -1.8601e-01, -1.9263e+00,  4.7374e-02,  1.0834e+00,\n",
      "         -1.3439e-01, -7.2158e-01, -1.2077e+00, -1.1400e+00,  7.8523e-01,\n",
      "         -6.0271e-01,  5.9434e-01,  1.1480e+00,  1.3064e+00, -1.2351e+00,\n",
      "         -1.3143e-02,  5.5804e-01, -2.1423e-01, -2.0733e+00, -5.7892e-01,\n",
      "          2.2787e-01,  8.2496e-01,  1.1500e+00,  6.6024e-02, -1.4289e+00,\n",
      "         -2.1709e-01,  3.7581e-02,  7.0504e-01, -2.9293e-01,  9.0244e-01,\n",
      "         -1.3608e+00,  1.3372e+00,  4.7717e-01, -7.2962e-03,  1.1003e-01,\n",
      "          5.5250e-02,  2.2012e-01, -2.5610e+00, -9.9155e-03, -5.9713e-01,\n",
      "         -7.9851e-01,  5.6308e-01,  7.5245e-01, -3.6169e-01,  1.4077e-01,\n",
      "          8.8215e-01, -9.1876e-01,  1.1132e+00, -4.8386e-01, -1.0062e+00,\n",
      "          6.7869e-02, -9.1209e-01,  1.7092e+00,  1.0601e+00, -5.5360e-01,\n",
      "         -1.2739e+00, -1.6388e+00, -1.9388e-01, -1.0426e+00,  1.7948e+00,\n",
      "         -2.3082e+00, -9.3144e-01, -3.6648e-01,  1.6004e+00, -2.1157e-01,\n",
      "          6.8052e-02, -2.9604e-01, -2.0326e-01,  1.1040e+00, -4.1084e-02,\n",
      "          2.2578e+00, -1.3996e+00, -5.8989e-02,  1.5884e+00, -3.3035e-01,\n",
      "          6.1155e-01,  1.0302e+00, -1.8550e+00, -1.4241e+00,  9.6636e-01,\n",
      "          1.1034e+00,  1.2569e+00, -3.3124e-01, -2.7279e-01, -9.2050e-02,\n",
      "          8.7690e-01, -1.0030e+00, -9.7440e-01, -1.4447e+00,  5.4476e-01,\n",
      "         -5.6564e-01,  4.2195e-01,  1.3481e-01, -4.8297e-01,  2.1778e+00,\n",
      "         -7.2426e-01,  1.0157e-01,  6.8975e-01, -1.4896e+00, -2.3128e-02,\n",
      "          2.0000e+00, -1.1098e+00, -2.3710e-01,  6.8489e-02,  9.3541e-01,\n",
      "          1.0394e+00,  7.8043e-01, -1.4651e+00,  2.7453e-03, -9.0683e-01,\n",
      "         -1.2774e+00,  4.5829e-01,  2.0451e-01,  1.2001e+00,  1.5761e+00,\n",
      "          3.9133e-01,  2.0559e-01,  1.5198e-01,  3.2140e-01, -1.4510e-01,\n",
      "         -7.4489e-01, -7.8660e-01, -9.8852e-01, -1.0399e+00,  6.6485e-01,\n",
      "         -2.0379e-01, -2.2371e-01,  4.2743e-01,  3.7299e-01, -4.6864e-01,\n",
      "         -7.7904e-03, -1.4943e+00,  1.4508e+00,  1.3321e+00,  1.4141e-01,\n",
      "         -5.0017e-01,  1.1242e+00, -2.2219e-01, -5.9514e-01,  3.0637e-01,\n",
      "          7.7907e-01,  8.9429e-01,  1.1607e+00,  2.0430e-01,  1.0195e+00,\n",
      "          1.3329e-01,  4.2839e-01, -2.0959e-01, -7.6461e-02,  2.0674e-01,\n",
      "          7.3925e-01, -1.8493e-01,  1.2141e+00,  8.2323e-02, -1.5004e-01,\n",
      "         -4.8065e-01,  1.0516e+00, -1.6947e-01, -6.5251e-01,  5.4286e-01,\n",
      "         -1.6949e+00,  4.6689e-01, -1.0366e-01,  1.1857e+00,  1.4029e-02,\n",
      "          1.3881e+00, -4.9334e-01,  1.0211e+00, -1.1869e+00, -4.2592e-01,\n",
      "         -9.0450e-01, -5.8545e-01,  1.8295e+00,  3.1235e-01,  1.6471e+00,\n",
      "          8.5126e-01,  9.7446e-02, -2.8600e+00, -3.0594e-01, -1.8763e-01,\n",
      "          1.1406e-01,  5.2224e-01, -9.3914e-02, -1.6910e-02,  3.1989e-01,\n",
      "          4.1123e-01, -1.1675e+00,  7.8745e-01,  3.4429e-01, -2.6602e-01,\n",
      "         -8.5207e-01, -1.4767e+00, -1.7776e+00, -2.1413e-01,  1.3795e-01,\n",
      "         -1.5108e-01, -2.9202e-01, -3.2609e-01, -5.5637e-01,  4.9724e-01,\n",
      "          6.4256e-01,  6.2467e-01,  2.3226e-01,  8.8578e-01, -3.5276e-02,\n",
      "         -9.1093e-01,  1.9879e+00, -2.4012e+00,  8.8484e-01,  1.0074e-01,\n",
      "          1.1979e+00,  1.4287e-01, -1.5467e+00, -7.5142e-01,  3.7934e-01,\n",
      "          1.6613e+00,  1.3644e+00,  3.3691e-01,  1.0182e+00,  2.1733e-01,\n",
      "          6.6265e-01,  1.0066e+00, -4.4956e-01,  4.7116e-01,  4.9209e-01,\n",
      "          5.5056e-01,  2.1917e+00,  3.3349e-01,  3.0492e-01, -4.6051e-01,\n",
      "          1.9387e+00,  7.2492e-02, -8.3640e-01,  2.4654e-01,  1.6294e+00,\n",
      "          9.2775e-01]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([4, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257 #gpt2 bytepair encoder has a vocabulary size of 50257\n",
    "output_dim = 256 # use a vector with 256 dims to represent a token\n",
    "\n",
    "#nn.Embedding本质是一个可训练的查找表，形状为vocab_size x output_dim\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "#inputs是一个batch_size x max_length的tensor，每个元素都是一个token的id\n",
    "#token_embeddings形状为batch_size x max_length x output_dim, 即每个token id都被映射为一个output_dim维的向量\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "\n",
    "\n",
    "#以上是一个查找的过程，例如token id = 40, 对应的embedding就是token_embedding_layer(torch.tensor([40]))\n",
    "#也就是token_embedding_layer[40][:]\n",
    "print(token_embedding_layer(torch.tensor([40])))\n",
    "\n",
    "\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "#如何构造四个token的位置关系？\n",
    "#构造一个4x256的embedding layer\n",
    "#输出代表距离的tensor [0,1,2,3]得到的就是position embedding\n",
    "\n",
    "\n",
    "context_length = max_length # = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "\n",
    "print(pos_embedding_layer.weight.shape)\n",
    "#pos_embedding_layer依然是一个embedding layer\n",
    "#相当于一个只有四个token id的embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "torch.Size([4, 4, 256])\n",
      "torch.Size([4, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "#距离为0,1,2,3的三个token对应的embedding\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "#广播相加：pos_embeddings被扩展为1x4x256\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
